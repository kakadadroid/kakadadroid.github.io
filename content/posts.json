[{"title": "Non-utf8 vs Mongodb", "timestamp": 1437592980.0, "modified": 1437593030, "filename": "11 Non-utf vs Mongodb", "content": "We saw 3 instances of a user agent string that broke our Python queries; It came with a non-utf8 encoded [\"orange.espana\"](https://en.wikipedia.org/wiki/Orange_Espa%C3%B1a).\n\n[Pymongo](https://pypi.python.org/pypi/pymongo/) throws an error (`UnicodeEncodeError`) when a query with those documents are returned. And we never get to see the id or anything of the related document. Scoping the query to logs from different time frame made the query go through, so we figured we just need to remove the offending document.\n\nTurns out it's actually really hard to find the offenders.\n\n`db.collection.find({\"user_agent\":/orange.esp/})` doesn't return the `\"orange.espana\" doc, because probably the non-utf8 string is skipped when searching for match.\n\nBut luckily the query only fails in the python client, and when we use the mongo console, we are able to print everything out, offending or not. So we did a binary search and narrowed down to a small enough scope that we can just scroll through and find the '?' and out of frustration we purged the entries.\n\nWth happened to the device that sent non-utf8 user agent?! And only 3 instances of them -.-\n\nIt was interesting that mongo didn't complain when we save non-utf8 strings in the document, through the [golang client](http://godoc.org/gopkg.in/mgo.v2); Gotta test that out again and document it. Now we do a unidecode on incoming user agent strings, just in case it would save us from squinting our eyes through hundreds of user agent strings again.", "post_id": "non-utf8-vs-mongodb"}, {"title": "The right tool for the right job", "timestamp": 1436382720.0, "modified": 1437106323, "filename": "10 The right tool for the right job", "content": "[deepo](http://www.deepo.io/) is upgrading our first minimal infrastructure.\n\n###We've Been Lazy\n\nWe used to be on Postgresql(RDS) + Rails(Heroku) + Golang(Heroku). It was simple to start with, but grew to be come a sluggish patchwork. The traffic that we deal with isn't huge by any standard, but we attempted to count uniques on some largish tables. We added master-slave replica to separate the writes and reads, created composite indexes on the queried columns, partitioned the table by date range, have background jobs update a summary table out of a star schema, but still we get very slow response time on the unique count queries.\n\nYes there's [HyperLogLog](https://en.wikipedia.org/wiki/HyperLogLog), and we've realise we should tolerate the 2% error. But we learned that the hard way. By now our partitioned slave throws a timeout on simple select, once in a while. I suspect it is because of my tinkering with column constraints without properly handling existing violations. Or it could be that rails migration of databases don't play nicely with master-slave set up. I have to do the same query on the master instance to avoid a timeout. \n\nOn top of the intermitten query problems, there was a pending design issue. Our postgresql acted like a poor man's persistent message queue. We have a `pending_page_views` table from which rows are waiting to be processed, then removed and inserted into the `page_views` table. Those extra writes and reads that results from the clumsy pipeline probably doesn't help the responsiveness of the database. It got the job done, and there were no extra technology to integrate. Simply postgres and rails sidekiq. Simple is good right?\n\nIn retrospect, it was simple only in terms of coding it out, as we didn't have to climb the learning curve of any other tools. But we were still trying to implement the same thing: persistent buffer queue for data stream processing (we get the time the processing happens too, so we can redo some jobs or even replay the whole thing). The complexity of the process is **the same**. We were not simplifying things, we were just lazy (well, being lazy is only a negative expession of trying to build as\nfast as possible with as little effort as possible, which is still a noble thing.)\n\n###SQL vs NoSQL\n\nAlong the way a couple of different experienced opinions sugguested we utilize a nosql database to take care of one part of the app, where writes is intensive, and queries are never joined. \n\n\"But, you mean I'll have to rewrite all my queries?!\" Yes. \n\nLuckily for us, it wasn't that painful at all. The `pending_page_views` and `page_views` aren't really \"relational\" data anyway. \nThey are more like [log data](http://docs.mongodb.org/ecosystem/use-cases/storing-log-data/) (who visit what page at what time). \n\nWe then have have Postgresql(RDS) + Rails(EC2) + MongoDB(EC2) + Golang(EC2). \n\nWe keep app configurations that face client admins as relational data. Postgresql for relational data, MongoDB for non-relational data. Sounds good! Although the complexity is now increased.\n\n###AMQP\n\nI've heard about [rabbitmq](https://www.rabbitmq.com/) before, but didn't really understand why I need it. Because I didn't. I probably still don't understand it. But I try: you can make a pub/sub thing, so we can queue and distribute the processing of incoming data stream to multile workers. Should fit the bill for our batch jobs on `pending_page_view`. It's an easy-to-setup AMQP protocal that persists messages; \nso if the server shuts down we can just restart and continue picking up jobs. (I finally understood [Apache Kafka](http://kafka.apache.org/) too)\n\nSo now: Postgresql(RDS) + Rails (EC2) + MongoDB(EC2) + Golang+Rabbitmq(EC2)\n\n###We need to run Python\n\nAs we become more mature on our data-science department, it became a requirement that we are able to run python scripts. We need to recalculate similarity indexes every time a new item enters the pool, and we need to reweight our message distribution weights periodically; these should be threaded. I almost fell into the rabbit hole of trying to let python manage the multiple threads. Instead we restrict python codes to what it does best: data science heavy lifting, and leave the concurrency management to the best guy: [Golang](https://www.youtube.com/watch?v=f6kdp27TYZs)\n\nFinally: Postgresql(RDS) + Rails (EC2) + MongoDB(EC2) + Golang+Rabbitmq+Python(EC2). Much complex! So effort!\n\n###Work in Progress\n\nSome decisions are probably not the best, and some obvious, but it's a starting point for me, questioning \"Is this the right tool for this job?\"; and everytime I've addressed that, I feel good. \nEveryone should feel good, so everyone should ask that question.\n\nHowever, the answer to \"is this the right tool for this job\" depends on what's available on your toolbelt. \nI've got only a handful and some of them I probably have wrong understandings of (like [AMQP rabbitmq](#)), but I'll be trying to aquire and sharpen more tools.", "post_id": "the-right-tool-for-the-right-job"}, {"title": "Subtle bug coming from Rails' first_or_create", "timestamp": 1420456320.0, "modified": 1436469858, "filename": "9 Subtle bug coming from Rails' first_or_create", "content": "One very useful method provided by Rails is [`first_or_create`](http://apidock.com/rails/v4.1.8/ActiveRecord/Relation/first_or_create)\n\nBut there's a substantial probability that one might use it like this:\n```\nModel.first_or_create(x: 1, y: 2)\n```\n<br>\nAt least I did at first. It caused a bug in that nothing ever gets created with `{x=1, y=2}`. This is because Model.first already returns an object - where(`id`=1) - so the create part doesn't get called.\n\nThe desired behaviour would be achieved by:\n\n```\nModel.where(x: 1, y: 2).first_or_create\n```", "post_id": "subtle-bug-coming-from-rails'-first_or_create"}, {"title": "AWS Key and Secret with special characters", "timestamp": 1420258980.0, "modified": 1436469858, "filename": "8 AWS Key and Secret with special characters", "content": "When playing around with file uploads using the gem paperclip from a heroku rails app ([I did as described here](https://devcenter.heroku.com/articles/paperclip-s3)), I got it working from my local environment, successfully uploading images to my bucket. But upon pushing to heroku production, the key and secret seems to have issues and I was given this:\n\n````\nAWS::S3::Errors::SignatureDoesNotMatch (The request signature we calculated does not match the signature you provided.\n````\nI tripple checked that there were no difference in the config variables and inspected the configured AWS credentials on production and all seemed to tally up.\n\nThen I found [this post that gave me some direction](http://stackoverflow.com/questions/2777078/amazon-mws-request-signature-calculated-does-not-match-the-signature-provided). Many solved by the trailing slashs and encoding of \"+\" and stuff in the credentials. I went ahead to the AWS console and kept refreshing till I get an all alpha-numeric key and secret pair. I got it after about 5 rounds of create-and-delete cycles. Magically the upload succeeds at first try.\n\nI'll never be sure that it's heroku's environment messing up the encoding special characters, but now it works and I'll keep using the alpha-numeric-only key.", "post_id": "aws-key-and-secret-with-special-characters"}, {"title": "RegEx is fascinating", "timestamp": 1406216040.0, "modified": 1436469858, "filename": "7. RegEx is fascinating.md", "content": "A few different projects called for the use of regex and until now, I still let go of a smile in amazement as I construct each regex query.\n\nA online tool that is super useful is [Regex 101](http://regex101.com/), where different flavour of regex is accessible at a toggle. There is also an **Explanation** box that breaks down the qurey that I just constructed by trial and error.\n\nWhile it's extremely useful and fulfilling, I still could not find enough reason and opportunity to learn regex proper. Just like many other things.\n\n**A cool app idea:**  Take a test string, and the desired matching part, and outputs the regex expression.", "post_id": "regex-is-fascinating"}, {"title": "2048 bot", "timestamp": 1394954580.0, "modified": 1436469858, "filename": "6 2048 bot.md", "content": "**Up-update:** I [forked the 2048 game](http://keang.me/2048) and added a looper feature, where anyone can create their stupid bot!\n\n**Update:** I stumbled upon this [super robust AI]( http://ov3y.github.io/2048-AI/) which solves the game! albiet with a bit of lag.\n\nIn the past 2 days my facebook newsfeed flooded with posts about a new game, [2048](http://gabrielecirulli.github.io/2048/) which itself is a clone from an existing game concept ([dejavu?](http://en.wikipedia.org/wiki/Flappy_Bird)).\n\nAfter playing with it for awhile I wanted to write an AI that solves the game, because this naturalI certainly can't solve this game! The first step I took was to simulate keypresses, which I did using a [js script here](https://gist.github.com/keang/9502722) in firebug console, so I didn't have to modify the game's code at all. At this point I still haven't read a single line of the source code. But all this does is either moving randomly, or moving in a fixed circular pattern.\n\nIt's a stupid bot that presses some keys for you, but it scores better than me! Will come back and work on a proper AI soon!", "post_id": "2048-bot"}, {"title": "End to end application: Follodota", "timestamp": 1394692200.0, "modified": 1437106632, "filename": "5 End to end application Follodota.md", "content": "I've always wanted to build something related to DotA2, because it's such an awesome game and I've been spending at least 5 hours a week clicking away playing DotA. So for the past 3 weeks I've been working on a video aggregator app, [follodota](https://play.google.com/store/apps/details?id=com.follodota&hl=en), which consolidates the English commentated matches uploaded by joinDota and beyondTheSummit.\n\nThis is an end-to-end application that I built while learning on the job: a python script which crawls the dotacinema.com website for new content, a rails app as server hosted on Heroku to store and serve the match information with json api, and of course an android native app to play those videos.\n\nCurrently it is still in its infancy, and I still have to run the python script every now and then. The script only gets 15 entries because the [beautiful soup](http://www.crummy.com/software/BeautifulSoup/bs4/doc/) + [request](http://docs.python-requests.org/en/latest/) combo in python could not emulate a scrolling down event, which is needed to load more videos on [dotacinema.com](www.dotacinema.com/vod). Much more work to do :P\n\nBut for now, you can search the matches chronologically, by teams and by leagues (and easily by casters, but that's not implemented yet). My hope would be to get this thing ready for TI4!([last year's TI here](http://www.dota2.com/international/home/overview/))\n\n<ul class=\"todo\">Todo:\n<li class=\"todoitem\">add in some Gimmie awesomeness in gamification</li>\n<li class=\"todoitem\">an iOS client would be great too </li>\n<li class=\"todoitem\">and also to crawl for more matches. </li>\n</ul>\n\nMaybe a script to automatically run the crawling script (which automatically posts matches to through the api). autoception. hah.", "post_id": "end-to-end-application:-follodota"}, {"title": "Export resources for Android with AdobeScript", "timestamp": 1394088000.0, "modified": 1436469858, "filename": "4. Exporting resources.md", "content": "In my learning journey as an Android developer, one annoying task is to export a graphic resource multiple times for the handful of screen resolutions that I want to support: mdpi, hdpi, xhdpi and now xxhdpi. This task involves a repeated set of mouse clicks, a change in scaling value, and navigating to the right folders for the right dpi. It can get quite tedious, especially when I often have multiple images for one button, and a gazillion edits before I'm happy with how it looks.\n\nThis calls for some scripting for the awesomeness of automation. There's something magical about automatic processes that fascinates me. So I wrote [this Adobe Script in this gist,](https://gist.github.com/keang/8701058) which has saved myself uncountable mouse clicks and hair pullings.\n\nI remember clearly the joy of running the script and watching the res/drawable/ subfolders be populated with the awesome crisp graphics that I just made. Visually witnessing the .png's popping up is magical, but knowing that you've created that magic is even more... _\"magicaller\"_.\n\nI'll probably visit this again and add in the dimensions for iOS resources as well when I get my hands on a Mac and have some time to learn iOS too.", "post_id": "export-resources-for-android-with-adobescript"}, {"title": "Rspec'ing a rials JSON Api", "timestamp": 1393832700.0, "modified": 1436469858, "filename": "3. Rspec'ing JSON Api", "content": "`render_view` was the missing piece:\nbecause rspec by default disables rendering of views to speed up the test. And that's how I was stuck for days trying to find bugs in the super short json.\n\n```ruby\n    describe Api::V1::TeamsController do\n\t\trender_views\n\t\tit \"returns a json array of teams\" do\n\t\t...\n        json = JSON.parse(response.body)\n        expect(json['teams']).not_to be_nil\n        end\n    end\n```", "post_id": "rspec'ing-a-rials-json-api"}, {"title": "Joining the Flappy craze", "timestamp": 1393738380.0, "modified": 1436469858, "filename": "2.flappy-quiz.md", "content": "**tl;dr**: my flappy clone [here](http://keang.github.io/flappy-quiz/).\n\nBeing swept by the recent craze for flappy bird, I wanted to pass on this little birdy frustration and at the same time make something fun. I love the fact that flappy bird is so frustratingly simple that to play you have to focus on some thing else other than the bird to do well. Some people suggesting looking at the skies behind. Some at the moving pipes. Others go further to add a [typing training in there](http://www.mrspeaker.net/dev/game/flappy/).\n\nOne interesting version I found was this awesome [flappy maths saga](tikwid/flappy-math-saga) which puts your multiplication power to the test. It's beautiful because the attention split needed there really proves as a good brain teaser. I thought this could extend to other fields as well, so I wanted to make a frustrating quiz engin for multiple choice questions. But the pipe is only so long, so only true/false type questions and 2-choices were possible.\n\nI told this to my girlfriend and she said her english class full of 13yo's will love it. We used google spreadsheet to compile a small quiz, which I was planning to hardcode into the modified html flappy. But while sharing the google spreadsheet I remembered that it could be made a public link, and so there should be a way to read the content of the public spreadsheet directly from the html.\n\n>\"The (sec2) kids went bananas!\"\n\nTurns out google spreadsheet do have an API, but the documentation is scarce and full of redirects between a few versions. From [this example](https://developers.google.com/gdata/samples/spreadsheet_sample) I figured out how to read the columns of a public spreadsheet, and voila, flappy-quiz just got its sheety backend wired up.\n\n````javascript\n\t//load questions\n\t$.getJSON(\"http://cors.io/spreadsheets.google.com/feeds/list\n\t\t/[google-sheet-uid]/od6/public/basic?alt=json\", function(data) {\n    \tvar rows = data.feed.entry;\n    \tfor(var i=0; i<rows.length; i++) {\n        \tvar t = rows[i].content.$t.split(/[ ,]+/);\n        \tvar question = {word:rows[i].title.$t, correct:t[1], wrong:t[3]};\n        \tquestions.push(question);\n\t\t}\n\t\tisQuestionLoaded = true;\n\t});\n````\n\n\nNow anyone with the spreadsheet link(and with permission) can update it, and the flappy bird will have a different problem to solve to go through the pipes!\n\nSee it [flapping live here](http://keang.github.io/flappy-quiz/) and if you want to, [fork from here!](http://keang.github.io/flappy-quiz/)", "post_id": "joining-the-flappy-craze"}, {"title": "blog commit -m \"init\"", "timestamp": 1393649160.0, "modified": 1436469858, "filename": "1. post-1.md", "content": "This is yet another blog that I shall promise myself to keep. This one will talk about the projects and cool and uncool things that I've done.\n\nI saw a friend's [super cool blog](http://yangshun.im/cs3216/#/) and after a few minutes of picking around I found his little python-based page generators for blogs, [luna](https://github.com/yangshun/luna). I just lost all the excuses to delay this. Mainly because I just wanted to try out this 'blogging engin'.\n\nSeems pretty cool! Because of this I had to install a LAMP stack and run on localhost to preview every edit, but it's a one time set up and now I can blog in markdown(for which I need to refer to [the cheatsheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet)).\n\nLet's get some shit done!", "post_id": "blog-commit--m-\"init\""}]